


import joblib
import numpy as np
import pandas as pd
import sklearn.ensemble as ek
import matplotlib.pyplot as plt
from IPython.display import display
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, learning_curve,  cross_val_score

# Load the dataset (adjust the file path if necessary)
dataset = pd.read_csv('Dataset/dataset_malwares.csv')
print("\nSetup complete")


total_size = dataset.shape[0]
print("Total number of samples:", total_size)





dataset['Malware'].value_counts(), dataset.isnull().sum().sum()  # The dataset contains: 14,599 malware samples (labeled as 1) and
                                                                 # 5,012 benign samples (labeled as 0)


dataset.head()    #Top 5 row of the dataset


dataset.tail()     #Last 5 row of the dataset


# summary of numeric attributes
dataset.describe(include="all")


# Display the DataFrame in a spreadsheet-like format
display(dataset)


X = dataset.drop(columns=['Malware'])
feature_names = X.columns  # Save the feature names
print(feature_names)


dataset.info()    # info about the whole dataset





dataset["Malware"].value_counts().plot(kind="pie",autopct="%1.1f%%")
plt.show()





X = dataset.drop(columns=['Name', 'Malware'])  #Droping this because classification model will not accept object type elements (float and int only)
y = dataset['Malware']





# Spliting the data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Initialize and train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
# Feature Importance
importances = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Print the feature importances
print(importance_df)

# Visualize Feature Importances
plt.figure(figsize=(12, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importances from Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y axis to show the most important features at the top
plt.show()


# Set the importance threshold
importance_threshold = 0.01

# Filter features based on the importance threshold
important_features = importance_df[importance_df['Importance'] > importance_threshold]['Feature']
print(f"Features selected (importance > {importance_threshold}):\n{important_features.values}")

# Visualize Feature Importances of Important Features
plt.figure(figsize=(12, 8))
plt.barh(importance_df['Feature'][importance_df['Importance'] > importance_threshold],
         importance_df['Importance'][importance_df['Importance'] > importance_threshold],
         color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importances from Random Forest Classifier (Filtered)')
plt.gca().invert_yaxis()  # Invert y axis to show the most important features at the top
plt.show()






# To see the dataset with only the important features
# Create a new dataset with only the important features
important_features_list = important_features.tolist()  # Convert to list
important_features_dataset = dataset[important_features_list]  # Filter the dataset

# save this filtered dataset to a CSV file
important_features_dataset.to_csv('filtered_important_features_dataset.csv', index=False)


# Display the list of selected important features, one per line
print("Selected Important Features (importance > 0.01):")
for feature in important_features:
    print(feature) 
print(len(important_features))





# Create training and testing datasets with only the important features
X_train_important = X_train[important_features]
X_test_important = X_test[important_features]


model = { "DecisionTree": DecisionTreeClassifier(max_depth=10),
         "RandomForest":ek.RandomForestClassifier(n_estimators=50)}


results = {}
for algo in model:
    clf = model[algo]
    clf.fit(X_train_important,y_train)
    score = clf.score(X_test_important,y_test)
    print ("%s : %s " %(algo, score))
    results[algo] = score


winner = max(results, key=results.get)# Selecting the classifier with good result
print("Using", winner, "for classification, with",len(important_features), 'features.')








# Train a new model with only the important features
rf_model_filtered = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model_filtered.fit(X_train_important, y_train)

# Predictions and Evaluation with important features
y_pred_filtered = rf_model_filtered.predict(X_test_important)
print(classification_report(y_test, y_pred_filtered, target_names=['Benign', 'Malware']))





# Save the New trained model to a file
joblib.dump(rf_model_filtered, 'malwareclassifier-V2.pkl')





# Split training data further into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)


# Evaluate training performance
train_scores = cross_val_score(rf_model, X_train, y_train, cv=5)
print("Training scores:", train_scores)
print("Average Training Score:", np.mean(train_scores))

# Evaluate validation performance through cross-validation
val_scores = cross_val_score(rf_model, X_val, y_val, cv=5)  
print("Validation scores:", val_scores)
print("Average Validation Score:", np.mean(val_scores))






train_sizes, train_scores, val_scores = learning_curve(rf_model, X_train, y_train, cv=5, n_jobs=-1)

# Compute the average and standard deviation of train/validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Plot learning curves
plt.plot(train_sizes, train_mean, label='Training Score')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")

plt.plot(train_sizes, val_mean, label='Validation Score')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color="g")

plt.xlabel('Training Set Size')
plt.ylabel('Score')
plt.legend(loc='best')
plt.show()



# Set bar width and positions
bar_width = 0.40
train_positions = np.arange(len(train_sizes))
val_positions = train_positions + bar_width

# Plot the bar chart with error bars
plt.bar(train_positions, train_mean, yerr=train_std, width=bar_width, label='Training Score', color='skyblue', capsize=5)
plt.bar(val_positions, val_mean, yerr=val_std, width=bar_width, label='Validation Score', color='lightgreen', capsize=5)

# Add labels and legend
plt.xlabel('Training Set Size')
plt.ylabel('Score')
plt.xticks(train_positions + bar_width / 2, train_sizes)
plt.legend(loc='best')
plt.title('Training and Validation Scores by Training Set Size')

# Show the plot
plt.show()



# Add this before the visualization code to understand the model structure
print(type(model))
if isinstance(model, dict):
    print("Keys in model dictionary:", model.keys())
    for key, value in model.items():
        print(f"Key: {key}, Type: {type(value)}")


# Add these imports at the top of your notebook
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
from sklearn.model_selection import cross_val_score, learning_curve
import numpy as np
import pandas as pd
import json

# After your model training section, add this code for metrics visualization

# 1. Basic Classification Metrics
def plot_classification_metrics(y_true, y_pred, y_scores=None):
    """Plot comprehensive classification metrics"""
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    # Create metrics dataframe for visualization
    metrics_df = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
        'Value': [accuracy, precision, recall, f1]
    })
    
    # Plot metrics
    plt.figure(figsize=(10, 6))
    ax = sns.barplot(x='Metric', y='Value', data=metrics_df)
    plt.title('Classification Metrics', fontsize=16)
    plt.ylim(0, 1.0)
    
    # Add values on top of bars
    for i, p in enumerate(ax.patches):
        ax.annotate(f'{p.get_height():.3f}', 
                    (p.get_x() + p.get_width() / 2., p.get_height()), 
                    ha='center', va='bottom', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))
    
    # ROC Curve if scores are provided
    if y_scores is not None:
        plt.figure(figsize=(8, 6))
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                 label=f'ROC curve (area = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.show()

# 2. Confusion Matrix Visualization
def plot_confusion_matrix(y_true, y_pred, classes=['Benign', 'Malware']):
    """Plot confusion matrix with percentages"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix', fontsize=16)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
    # Calculate and display percentages
    group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    
    plt.tight_layout()
    plt.show()
    
    # Calculate and print metrics from confusion matrix
    tn, fp, fn, tp = cm.ravel()
    total = np.sum(cm)
    
    print(f"True Negatives: {tn} ({tn/total:.2%})")
    print(f"False Positives: {fp} ({fp/total:.2%})")
    print(f"False Negatives: {fn} ({fn/total:.2%})")
    print(f"True Positives: {tp} ({tp/total:.2%})")
    
    accuracy = (tp + tn) / total
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nAccuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

# 3. Feature Importance Visualization
def plot_feature_importance(clf, feature_names, top_n=20):
    """Plot feature importance for the model"""
    if hasattr(clf, 'feature_importances_'):
        importances = clf.feature_importances_
    else:
        print("Model doesn't provide feature importances directly")
        return
    
    # Create dataframe of features and their importance
    feature_importance = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    })
    
    # Sort by importance
    feature_importance = feature_importance.sort_values('Importance', ascending=False)
    
    # Display top N features
    top_features = feature_importance.head(top_n)
    
    plt.figure(figsize=(10, 8))
    sns.barplot(x='Importance', y='Feature', data=top_features)
    plt.title(f'Top {top_n} Feature Importance', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    return feature_importance

# Fix the usage example - use your actual model variable names:

# First, access your classifier from the model dictionary 
# (assuming your structure, modify as needed)
classifier = model['malwareclassifier.pkl']  # Or whatever key contains your actual model

# Get predictions
y_pred = classifier.predict(X_test)
y_scores = classifier.predict_proba(X_test)[:,1]

# Plot all classification metrics
plot_classification_metrics(y_test, y_pred, y_scores)

# Plot confusion matrix
plot_confusion_matrix(y_test, y_pred)

# Plot feature importance
plot_feature_importance(classifier, X_train.columns)

# Cross-validation can be done on the classifier
cv_scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')
print(f"CV Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}")

# Save metrics to file for future reference
metrics_results = {
    'accuracy': accuracy_score(y_test, y_pred),
    'precision': precision_score(y_test, y_pred),
    'recall': recall_score(y_test, y_pred),
    'f1': f1_score(y_test, y_pred),
    'cv_scores': list(cv_scores.astype(float))
}

with open('Dataset/model_metrics.json', 'w') as f:
    json.dump(metrics_results, f, indent=4)



